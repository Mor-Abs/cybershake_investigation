{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code Description:\n",
    "This script performs the following tasks to consolidate and manage IM (Intensity Measure) data \n",
    "from existing and new realizations:\n",
    "\n",
    "1. **Data Consolidation**:\n",
    "   - Combines all IM values from existing and new realizations into a newly generated combined directory.\n",
    "   - Identifies common stations and IM types for each source (fault) across old and new realizations.\n",
    "   - Filters out uncommon stations and IM types to ensure consistency.\n",
    "\n",
    "2. **Median Calculation**:\n",
    "   - Computes the median IM values for each station and IM type from all realizations.\n",
    "   - Saves the computed median values as a CSV file.\n",
    "\n",
    "3. **Source File Management**:\n",
    "   - Copies all \"Source\" files from old and new realizations into the \"Source\" directory in the combined folder.\n",
    "   - Copies `nzvm.cfg` and `vm_params.yaml` files from the old realizations as-is.\n",
    "   - Copies these files from the new realizations with a \"_new\" suffix added to the filenames for differentiation.\n",
    "\n",
    "4. **Summary Reports**:\n",
    "   - Generates summary reports, including an Excel file, that details the number of stations and IMs processed.\n",
    "   - Adds a note file providing context and metadata about the combined data.\n",
    "\n",
    "Author: Morteza\n",
    "Version History:\n",
    "- Version 1.0: August 18, 2024\n",
    "- Version 1.1: December 18, 2024\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path and directory works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User input source folder name for Existing Realizations is: v20p6\n",
      "User input source folder name for New Realizations is: v24p8\n",
      "Deleted and renewed the path: \n",
      "/mnt/hypo_data/mab419/Cybershake_Data/combined_v20p6_v24p8\n"
     ]
    }
   ],
   "source": [
    "file_path = os.getcwd()\n",
    "root_path = \"/\".join(file_path.split(\"/\")[:4])\n",
    "\n",
    "# Get source folder name as user input\n",
    "# Existing_RL_Name = input(\"Please Input the Source Folder Name of Existing Realizations: \")\n",
    "# New_RL_Name = input(\"Please Input the Source Folder Name of New Realization: \")\n",
    "Existing_RL_Name = 'v20p6'\n",
    "New_RL_Name = 'v24p8'\n",
    "\n",
    "# Print the result\n",
    "print(f\"User input source folder name for Existing Realizations is: {Existing_RL_Name}\")\n",
    "print(f\"User input source folder name for New Realizations is: {New_RL_Name}\")\n",
    "\n",
    "\n",
    "old_cs_files_path = os.path.join(root_path, 'Cybershake_Data', Existing_RL_Name)\n",
    "new_cs_files_path = os.path.join(root_path, 'Cybershake_Data', New_RL_Name)\n",
    "combined_cs_files_path = os.path.join(\n",
    "    root_path, \"Cybershake_Data\", f\"combined_{Existing_RL_Name}_{New_RL_Name}\"\n",
    ")\n",
    "\n",
    "# Check if the directory exists\n",
    "if Path(combined_cs_files_path).exists():\n",
    "    prompt = input(\n",
    "        f\"The following path already exists! Do you want to delete and renew (1) or terminate (2)? \\n{combined_cs_files_path}\\n\"\n",
    "    )\n",
    "    if prompt == \"1\":\n",
    "        # Remove the folder\n",
    "        shutil.rmtree(combined_cs_files_path)\n",
    "        print(f\"Deleted and renewed the path: \\n{combined_cs_files_path}\")\n",
    "        os.makedirs(combined_cs_files_path, exist_ok=False)\n",
    "    elif prompt == \"2\":\n",
    "        print(\"Terminating the process.\")\n",
    "        exit()\n",
    "    else:\n",
    "        print(\"Invalid input. Terminating the process.\")\n",
    "        exit()\n",
    "else:\n",
    "    os.makedirs(combined_cs_files_path, exist_ok=False)\n",
    "    print(f\"Created the path: \\n{combined_cs_files_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both realization directories share the same faults!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 478/478 [1:53:00<00:00, 14.19s/it]  \n"
     ]
    }
   ],
   "source": [
    "def strip_and_convert(value):\n",
    "    \"\"\"\n",
    "    Strips leading/trailing whitespace or single quotes from a value\n",
    "    and converts it to a float if possible.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return float(str(value).strip(\"'\"))\n",
    "    except ValueError:\n",
    "        return value  # Return as-is if it can't be converted\n",
    "\n",
    "\n",
    "summary_data = []\n",
    "station_im_data = []\n",
    "\n",
    "im_true_order = [\n",
    "    \"PGA\",\n",
    "    \"PGV\",\n",
    "    \"CAV\",\n",
    "    \"AI\",\n",
    "    \"Ds575\",\n",
    "    \"Ds595\",\n",
    "    \"MMI\",\n",
    "    \"pSA_0.01\",\n",
    "    \"pSA_0.02\",\n",
    "    \"pSA_0.03\",\n",
    "    \"pSA_0.04\",\n",
    "    \"pSA_0.05\",\n",
    "    \"pSA_0.075\",\n",
    "    \"pSA_0.1\",\n",
    "    \"pSA_0.12\",\n",
    "    \"pSA_0.15\",\n",
    "    \"pSA_0.17\",\n",
    "    \"pSA_0.2\",\n",
    "    \"pSA_0.25\",\n",
    "    \"pSA_0.3\",\n",
    "    \"pSA_0.4\",\n",
    "    \"pSA_0.5\",\n",
    "    \"pSA_0.6\",\n",
    "    \"pSA_0.7\",\n",
    "    \"pSA_0.75\",\n",
    "    \"pSA_0.8\",\n",
    "    \"pSA_0.9\",\n",
    "    \"pSA_1.0\",\n",
    "    \"pSA_1.25\",\n",
    "    \"pSA_1.5\",\n",
    "    \"pSA_2.0\",\n",
    "    \"pSA_2.5\",\n",
    "    \"pSA_3.0\",\n",
    "    \"pSA_4.0\",\n",
    "    \"pSA_5.0\",\n",
    "    \"pSA_6.0\",\n",
    "    \"pSA_7.5\",\n",
    "    \"pSA_10.0\",\n",
    "]\n",
    "\n",
    "\n",
    "faults_old = [\n",
    "    cur_dir.stem for cur_dir in Path( ).iterdir() if cur_dir.is_dir()\n",
    "]\n",
    "faults_new = [\n",
    "    cur_dir.stem for cur_dir in Path(new_cs_files_path).iterdir() if cur_dir.is_dir()\n",
    "]\n",
    "\n",
    "# Check if both fault lists are the same\n",
    "if faults_old == faults_new:\n",
    "    print(\"Both realization directories share the same faults!\")\n",
    "else:\n",
    "    print(\"!!!! Caution !!!! >> Faults in the introduced directories are not the same!\")\n",
    "    prompt = input(\"Continue (1) or Terminate (2)? \")\n",
    "    if prompt == \"2\":\n",
    "        print(\"Terminating the process.\")\n",
    "        exit()\n",
    "    elif prompt != \"1\":\n",
    "        print(\"Invalid input. Terminating the process.\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "for cur_fault in tqdm(faults_old):\n",
    "\n",
    "    pure_im_data_list = []\n",
    "    old_file_counter = 0\n",
    "    new_file_counter = 0\n",
    "\n",
    "    ########################### working on IM files ###########################\n",
    "    cur_old_im_files = list(\n",
    "        (Path(old_cs_files_path) / cur_fault / \"IM\").rglob(\"*REL*.csv\")\n",
    "    )\n",
    "    cur_new_im_files = list(\n",
    "        (Path(new_cs_files_path) / cur_fault / \"IM\").rglob(\"*REL*.csv\")\n",
    "    )\n",
    "\n",
    "    if not cur_old_im_files or not cur_new_im_files:\n",
    "        print(f\"No matching files found for fault: {cur_fault}\")\n",
    "        continue\n",
    "\n",
    "    # Read the first file from old realizations\n",
    "    first_old_file = cur_old_im_files[0]\n",
    "    old_im_df = pd.read_csv(first_old_file)\n",
    "\n",
    "    # Extract stations and IMs from the old realization\n",
    "    old_stations = set(old_im_df[old_im_df[\"component\"] == \"geom\"][\"station\"])\n",
    "    old_ims = set(old_im_df.columns[2:])\n",
    "    corrected_ims_old = {\n",
    "        im[:5] + im[5:].replace(\"p\", \".\", 1) if im.startswith(\"pSA_\") else im\n",
    "        for im in old_ims\n",
    "    }\n",
    "\n",
    "    # Read the first file from new realizations\n",
    "    first_new_file = cur_new_im_files[0]\n",
    "    new_im_df = pd.read_csv(first_new_file)\n",
    "\n",
    "    # Extract stations and IMs from the new realization\n",
    "    new_stations = set(new_im_df[new_im_df[\"component\"] == \"geom\"][\"station\"])\n",
    "    new_ims = set(new_im_df.columns[2:])\n",
    "    corrected_ims_new = {\n",
    "        im[:5] + im[5:].replace(\"p\", \".\", 1) if im.startswith(\"pSA_\") else im\n",
    "        for im in old_ims\n",
    "    }\n",
    "\n",
    "    # Find common stations and IMs\n",
    "    common_stations = old_stations & new_stations\n",
    "    common_ims = corrected_ims_old & corrected_ims_new\n",
    "    common_ims_list = sorted(list(common_ims), key=im_true_order.index)\n",
    "\n",
    "    # Process old realizations\n",
    "    for cur_im_file in cur_old_im_files:\n",
    "        # Read the current IM file\n",
    "        cur_im_df = pd.read_csv(\n",
    "            cur_im_file,\n",
    "            converters={\n",
    "                col: strip_and_convert for col in range(2, len(common_ims_list) + 5)\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # Filter to retain only common stations and common IMs\n",
    "        temp_df1 = cur_im_df[\n",
    "            (cur_im_df[\"station\"].isin(common_stations))\n",
    "            & (cur_im_df[\"component\"] == \"geom\")\n",
    "        ]\n",
    "\n",
    "        temp_df1.columns = [\n",
    "            (\n",
    "                col[:5] + col[5:].replace(\"p\", \".\", 1)\n",
    "                if col.startswith(\"pSA_\") and \"p\" in col[5:]\n",
    "                else col\n",
    "            )\n",
    "            for col in temp_df1.columns\n",
    "        ]\n",
    "\n",
    "        filtered_im_df = temp_df1[[\"station\", \"component\"] + common_ims_list]\n",
    "\n",
    "        #  Convert data to xarray format\n",
    "        temp_df = filtered_im_df.set_index(\"station\").drop(\"component\", axis=1)\n",
    "\n",
    "        temp_da = xr.DataArray(\n",
    "            temp_df.values,\n",
    "            dims=(\"station\", \"IM\"),  # Define dimensions\n",
    "            coords={\n",
    "                \"station\": temp_df.index,\n",
    "                \"IM\": temp_df.columns,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        pure_im_data_list.append(temp_da)\n",
    "\n",
    "        # Define the target file path\n",
    "        target_im_file = (\n",
    "            Path(combined_cs_files_path) / cur_fault / \"IM\" / cur_im_file.name\n",
    "        )\n",
    "        target_im_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save the filtered data to the target file\n",
    "        filtered_im_df.to_csv(target_im_file, index=False)\n",
    "\n",
    "        # Update counters and summary\n",
    "        old_file_counter += 1\n",
    "        summary_data.append(\n",
    "            {\n",
    "                \"Source\": cur_fault,\n",
    "                \"RL\": cur_im_file.stem,\n",
    "                \"RL_Name\": Existing_RL_Name,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Process new realizations\n",
    "    for cur_im_file in cur_new_im_files:\n",
    "        # Read the current IM file\n",
    "        cur_im_df = pd.read_csv(\n",
    "            cur_im_file, converters={col: strip_and_convert for col in range(2, len(common_ims_list) + 5)}\n",
    "        )\n",
    "\n",
    "        # Filter to retain only common stations and common IMs\n",
    "        temp_df1 = cur_im_df[\n",
    "            (cur_im_df[\"station\"].isin(common_stations))\n",
    "            & (cur_im_df[\"component\"] == \"geom\")\n",
    "        ]\n",
    "\n",
    "        temp_df1.columns = [\n",
    "            (\n",
    "                col[:5] + col[5:].replace(\"p\", \".\", 1)\n",
    "                if col.startswith(\"pSA_\") and \"p\" in col[5:]\n",
    "                else col\n",
    "            )\n",
    "            for col in temp_df1.columns\n",
    "        ]\n",
    "\n",
    "        filtered_im_df = temp_df1[[\"station\", \"component\"] + common_ims_list]\n",
    "\n",
    "        #  Convert data to xarray format\n",
    "        temp_df = filtered_im_df.set_index(\"station\").drop(\"component\", axis=1)\n",
    "\n",
    "        temp_da = xr.DataArray(\n",
    "            temp_df.values,\n",
    "            dims=(\"station\", \"IM\"),  # Define dimensions\n",
    "            coords={\n",
    "                \"station\": temp_df.index,\n",
    "                \"IM\": temp_df.columns,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        pure_im_data_list.append(temp_da)\n",
    "\n",
    "        # Define the target file path\n",
    "        target_im_file = (\n",
    "            Path(combined_cs_files_path) / cur_fault / \"IM\" / cur_im_file.name\n",
    "        )\n",
    "        target_im_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save the filtered data to the target file\n",
    "        filtered_im_df.to_csv(target_im_file, index=False)\n",
    "\n",
    "        # Update counters and summary\n",
    "        new_file_counter += 1\n",
    "        summary_data.append(\n",
    "            {\n",
    "                \"Source\": cur_fault,\n",
    "                \"RL\": cur_im_file.stem,\n",
    "                \"RL_Name\": New_RL_Name,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Combine all data into a single xarray Dataset\n",
    "    combined_da = xr.concat(pure_im_data_list, dim=\"file_id\")\n",
    "\n",
    "    # Create Dataset with labeled dimensions\n",
    "    im_data_ds = xr.Dataset(\n",
    "        {\"IM_values\": combined_da},\n",
    "        coords={\n",
    "            \"station\": list(common_stations),\n",
    "            \"IM\": common_ims_list,\n",
    "            \"file_id\": range(old_file_counter + new_file_counter),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    im_data_ds[\"IM_values\"].values = im_data_ds[\"IM_values\"].values.astype(float)\n",
    "\n",
    "    # compute the median value and save the median file\n",
    "    # Step 1: Calculate the median across 'file_id'\n",
    "    median_2d_array = im_data_ds[\"IM_values\"].median(dim=\"file_id\")\n",
    "\n",
    "    # Step 2: Convert to DataFrame and prepare the output format\n",
    "    median_df = pd.DataFrame(\n",
    "        median_2d_array.values, \n",
    "        index=im_data_ds[\"station\"].values, \n",
    "        columns=im_data_ds[\"IM\"].values\n",
    "    )\n",
    "\n",
    "    # Step 3: Add 'station' and 'component' columns\n",
    "    median_df.insert(0, \"station\", median_df.index)  # Insert 'station' as the first column\n",
    "    median_df.insert(1, \"component\", \"geom\")         # Add 'component' column with \"geom\" value\n",
    "\n",
    "    # Step 4: Write the median DataFrame to a CSV file\n",
    "    median_im_file = (\n",
    "        Path(combined_cs_files_path) / cur_fault / \"IM\" / f\"{cur_fault}.csv\"\n",
    "    )\n",
    "    median_df.to_csv(median_im_file, index=False)\n",
    "\n",
    "    station_im_data.append(\n",
    "        {\n",
    "            \"Source\": cur_fault,\n",
    "            \"N_Old_stations\": len(old_stations),\n",
    "            \"N_New_Stations\": len(new_stations),\n",
    "            \"N_Comb_Stations\": len(common_stations),\n",
    "            \"N_Old_IMs\": len(old_ims),\n",
    "            \"N_New_IMs\": len(new_ims),\n",
    "            \"N_Comb_IMs\": len(common_ims),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    ########################### working on Source files ###########################\n",
    "    cur_old_source_dir = Path(old_cs_files_path) / cur_fault / \"Source\"\n",
    "    cur_comb_source_dir = Path(combined_cs_files_path) / cur_fault / \"Source\"\n",
    "\n",
    "    shutil.copytree(cur_old_source_dir, cur_comb_source_dir, dirs_exist_ok=True)\n",
    "\n",
    "    cur_new_source_dir = Path(new_cs_files_path) / cur_fault / \"Source\"\n",
    "    for file in cur_new_source_dir.glob(\"*.*\"):  # Match all files\n",
    "        if file.suffix in [\".csv\", \".info\"]:  # Filter for .csv and .info files\n",
    "            shutil.copy2(file, cur_comb_source_dir)\n",
    "        elif file.suffix in [\".cfg\", \".yaml\"]:\n",
    "            new_file_name = f\"{file.stem}_new{file.suffix}\"\n",
    "            shutil.copy2(file, cur_comb_source_dir / new_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary data saved to /mnt/hypo_data/mab419/Cybershake_Data/combined_v20p6_v24p8/summary.csv\n",
      "Stations and IMs data saved to /mnt/hypo_data/mab419/Cybershake_Data/combined_v20p6_v24p8/station_im_summary.csv\n",
      "Note written to /mnt/hypo_data/mab419/Cybershake_Data/combined_v20p6_v24p8/note.txt\n"
     ]
    }
   ],
   "source": [
    "# Convert summary data to DataFrame\n",
    "summary_df = pd.DataFrame(summary_data).sort_values(by=\"Source\")\n",
    "\n",
    "# Create the final summary table\n",
    "final_summary = summary_df.groupby([\"Source\", \"RL_Name\"]).size().unstack(fill_value=0)\n",
    "final_summary[\"Total\"] = final_summary.sum(axis=1)\n",
    "\n",
    "# Add the overall totals row\n",
    "overall_totals = final_summary.sum().rename(\"All\")\n",
    "final_summary = pd.concat([overall_totals.to_frame().T, final_summary])\n",
    "\n",
    "# Save to CSV\n",
    "summary_file_path = os.path.join(combined_cs_files_path, \"summary.csv\")\n",
    "final_summary.to_csv(summary_file_path)\n",
    "print(f\"Summary data saved to {summary_file_path}\")\n",
    "\n",
    "# Convert station_im_summary data to DataFrame\n",
    "station_im_summary_df = pd.DataFrame(station_im_data).sort_values(by=\"Source\")\n",
    "\n",
    "# Save to CSV\n",
    "station_im_summary_file_path = os.path.join(combined_cs_files_path, \"station_im_summary.csv\")\n",
    "station_im_summary_df.to_csv(station_im_summary_file_path)\n",
    "print(f\"Stations and IMs data saved to {station_im_summary_file_path}\")\n",
    "\n",
    "# Write Note\n",
    "Current_Date_and_Time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "note_file_path = os.path.join(combined_cs_files_path, 'note.txt')\n",
    "\n",
    "note_content = (\n",
    "    f\"- This folder contains combined IM (Intensity Measure) data from {Existing_RL_Name} and {New_RL_Name} \"\n",
    "    f\"realizations of the Cybershake project.\\n\"\n",
    "    f\"- Date of generation: {Current_Date_and_Time}\\n\"\n",
    "    f\"- Key Notes:\\n\"\n",
    "    f\"    1. Only IM values common to both old and new realizations have been combined.\\n\"\n",
    "    f\"    2. For each fault (source), only the common stations and IM types between the realizations have been retained.\\n\"\n",
    "    f\"    3. Median IM values for each station and IM type have been computed and saved as a separate CSV file.\\n\"\n",
    "    f\"    4. Source files have been copied:\\n\"\n",
    "    f\"       - 'nzvm.cfg' and 'vm_params.yaml' from the old realizations are included as-is.\\n\"\n",
    "    f\"       - Corresponding files from the new realizations have been copied with a '_new' suffix.\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "# Write to file\n",
    "with open(note_file_path, \"w\") as file:\n",
    "    file.write(note_content)\n",
    "\n",
    "print(f\"Note written to {note_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "morteza",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
